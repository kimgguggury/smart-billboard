import cv2
import time
import os
import json
import requests
from deepface import DeepFace
from retinaface import RetinaFace
import dlib
import numpy as np

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
CAPTURE_FOLDER = "captured_images"
os.makedirs(CAPTURE_FOLDER, exist_ok=True)
analysis_result_path = os.path.join(CAPTURE_FOLDER, "analysis_result.json")
current_ad_path = "current_ad.json"  # í˜„ìž¬ ê´‘ê³  ID íŒŒì¼

# dlib ê°ìœ„ ê²€ìƒ‰ê¸° ë° ëžœë“œë§ í´ëž˜ìŠ¤ ë¡œë“œí•´ì„œ ë³´ìœ 

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")

def get_current_ad_id():
    try:
        with open(current_ad_path, "r", encoding="utf-8") as f:
            return json.load(f).get("ad_id")
    except:
        print("âŒ current_ad.json íŒŒì¼ ì—†ìŒ")
        return None

def analyze_image():
    camera = cv2.VideoCapture(0)
    if not camera.isOpened():
        print("âŒ ì¹´ë©”ë¼ ì—´ ìˆ˜ ì—†ìŒ")
        return

    print("âœ… ì‹œìž‘: ì—´ ë²ˆì§¸ ê³ ê¸‰ì‹œ ê²€ì‚¬")

    try:
        while True:
            success, frame = camera.read()
            if not success:
                print("âŒ ì¹´ë©”ë¼ í”„ë§¤ìž„ ì½ì„ ìˆ˜ ì—†ìŒ")
                time.sleep(1)
                continue

            # 1. DeepFace + RetinaFace ì¶”ì 
            results = RetinaFace.detect_faces(frame)
            current_data = []

            for face_id in results:
                face_info = results[face_id]
                x1, y1, x2, y2 = face_info["facial_area"]
                face_img = frame[y1:y2, x1:x2]
                if face_img.size == 0:
                    continue
                try:
                    result = DeepFace.analyze(
                        face_img,
                        actions=["age", "gender"],
                        detector_backend="skip",
                        enforce_detection=False
                    )
                    analyzed_data = {
                        "age": int(result[0]["age"]),
                        "gender": result[0]["dominant_gender"]
                    }
                    current_data.append(analyzed_data)
                except Exception as e:
                    print("âŒ DeepFace ë¶„ì„ ì˜¤ë¥˜:", e)

            # 2. ê²°ê³¼ ì €ìž¥
            with open(analysis_result_path, "w", encoding="utf-8") as json_file:
                json.dump(current_data, json_file, ensure_ascii=False, indent=4)
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {analysis_result_path}")

            # 3. ì •ë©´ì´ë©´ analyze_view
            print("ðŸ•’ 15ì´ˆ ëŒ€ê¸° í›„ ì‹œì„  íŒë‹¨")
            time.sleep(7)
            analyze_view(camera)
            time.sleep(7)

    finally:
        camera.release()
        print("âœ… ì¹´ë©”ë¼ í•´ì œ")

def analyze_view(camera):
    success, frame = camera.read()
    if not success:
        print("âŒ ì¹´ë©”ë¼ í”„ë§¤ìž„ ì½ê¸° ì‹¤íŒ¨ (view)")
        return

    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)
    view_data = []

    for face in faces:
        shape = predictor(gray, face)
        image_points = np.array([
            (shape.part(30).x, shape.part(30).y),
            (shape.part(8).x, shape.part(8).y),
            (shape.part(36).x, shape.part(36).y),
            (shape.part(45).x, shape.part(45).y),
            (shape.part(48).x, shape.part(48).y),
            (shape.part(54).x, shape.part(54).y)
        ], dtype="double")

        model_points = np.array([
            (0.0, 0.0, 0.0),
            (0.0, -330.0, -65.0),
            (-225.0, 170.0, -135.0),
            (225.0, 170.0, -135.0),
            (-150.0, -150.0, -125.0),
            (150.0, -150.0, -125.0)
        ])

        size = frame.shape
        focal_length = size[1]
        center = (size[1] / 2, size[0] / 2)
        camera_matrix = np.array([
            [focal_length, 0, center[0]],
            [0, focal_length, center[1]],
            [0, 0, 1]
        ], dtype="double")
        dist_coeffs = np.zeros((4, 1))

        success, rotation_vector, translation_vector = cv2.solvePnP(
            model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE
        )

        rvec_matrix, _ = cv2.Rodrigues(rotation_vector)
        proj_matrix = np.hstack((rvec_matrix, translation_vector))
        _, _, _, _, _, _, euler_angles = cv2.decomposeProjectionMatrix(proj_matrix)

        pitch, yaw, roll = [angle[0] for angle in euler_angles]
        if pitch > 90: pitch = 180 - pitch
        elif pitch < -90: pitch = -180 - pitch

        print(f"Yaw: {yaw:.2f}, Pitch: {pitch:.2f}")

        if -20 <= yaw <= 20 and -15 <= pitch <= 15:
            print("âœ… ì •ë©´ìž„ - ë¶„ì„ ì‹œìž‘")
            x, y, w, h = face.left(), face.top(), face.width(), face.height()
            face_img = frame[y:y+h, x:x+w]
            try:
                result = DeepFace.analyze(face_img, actions=["age", "gender"], enforce_detection=False)
                view_data.append({
                    "age": int(result[0]["age"]),
                    "gender": result[0]["dominant_gender"]
                })
            except Exception as e:
                print("âŒ DeepFace ë¶„ì„ ì˜¤ë¥˜ (view):", e)
        else:
            print("âŒ ì•ˆë´„")

    # âœ… ì„œë²„ë¡œ view ì •ë³´ ì „ì†¡
    ad_id = get_current_ad_id()
    if ad_id and view_data:
        try:
            response = requests.post(
                "http://localhost:5000/api/viewed",
                json={"ad_id": ad_id, "people": view_data}
            )
            print("ðŸ“¡ view ì „ì†¡ ì™„ë£Œ:", response.status_code, response.text)
        except Exception as e:
            print("âŒ ì„œë²„ ì „ì†¡ ì‹¤íŒ¨:", e)
    else:
        print("âŒ ì •ë©´ ì—†ìŒ ë˜ëŠ” ê´‘ê³  ID ì—†ìŒ")

if __name__ == "__main__":
    analyze_image()
